/**
 * LLM Abstraction Types
 *
 * Provider-agnostic type definitions for LLM integration.
 * These types define the contract for LLM providers and tool calling,
 * enabling the LLMService to work with different backends.
 */

// =============================================================================
// Message Types
// =============================================================================

/**
 * Role of a message participant in a conversation.
 */
export type MessageRole = "user" | "assistant";

/**
 * A text content block containing plain text.
 */
export interface TextContentBlock {
  type: "text";
  text: string;
}

/**
 * A tool use content block representing a tool call from the assistant.
 */
export interface ToolUseContentBlock {
  type: "tool_use";
  /** Unique identifier for this tool use, used to match with tool results */
  id: string;
  /** Name of the tool being called */
  name: string;
  /** Input parameters for the tool call */
  input: Record<string, unknown>;
}

/**
 * A tool result content block containing the output of a tool execution.
 */
export interface ToolResultContentBlock {
  type: "tool_result";
  /** ID of the tool use this result corresponds to */
  toolUseId: string;
  /** Result content from the tool execution */
  content: string;
  /** Whether the tool execution resulted in an error */
  isError?: boolean;
}

/**
 * Union type for all content block types that can appear in messages.
 */
export type ContentBlock = TextContentBlock | ToolUseContentBlock | ToolResultContentBlock;

/**
 * A message in an LLM conversation.
 * Content can be a simple string or an array of content blocks for
 * complex interactions like tool use.
 */
export interface LLMMessage {
  role: MessageRole;
  content: string | ContentBlock[];
}

// =============================================================================
// Tool Types
// =============================================================================

/**
 * JSON Schema for tool input parameters.
 * Must be an object type with optional properties definition.
 */
export interface ToolInputSchema {
  type: "object";
  properties?: Record<string, unknown>;
  required?: string[];
  [key: string]: unknown;
}

/**
 * Definition of a tool that can be called by the LLM.
 */
export interface Tool {
  /** Unique name for the tool (used in tool_use blocks) */
  name: string;
  /** Description of what this tool does, used by the LLM to decide when to use it */
  description: string;
  /** JSON Schema defining the input parameters for this tool */
  inputSchema: ToolInputSchema;
}

/**
 * A request from the LLM to execute a tool.
 * Extracted from ToolUseContentBlock for convenience.
 */
export interface ToolCall {
  /** Unique identifier for this tool call */
  id: string;
  /** Name of the tool to execute */
  name: string;
  /** Input parameters for the tool */
  input: Record<string, unknown>;
}

/**
 * Result of executing a tool, to be sent back to the LLM.
 */
export interface ToolResult {
  /** ID of the tool call this result corresponds to */
  toolUseId: string;
  /** String content of the tool result */
  content: string;
  /** Whether the tool execution failed */
  isError?: boolean;
}

// =============================================================================
// Response Types
// =============================================================================

/**
 * Reason why the LLM stopped generating.
 * - end_turn: Natural completion
 * - tool_use: Model wants to call one or more tools
 * - max_tokens: Hit token limit
 * - stop_sequence: Hit a stop sequence (not used in our abstraction)
 */
export type StopReason = "end_turn" | "tool_use" | "max_tokens" | "stop_sequence";

/**
 * Token usage statistics from an LLM response.
 */
export interface LLMUsage {
  inputTokens: number;
  outputTokens: number;
}

/**
 * Response from an LLM provider.
 */
export interface LLMResponse {
  /** Content blocks generated by the model */
  content: ContentBlock[];
  /** Reason the model stopped generating */
  stopReason: StopReason;
  /** Token usage statistics (may not be available in all cases) */
  usage?: LLMUsage;
}

// =============================================================================
// Provider Interface
// =============================================================================

/**
 * Abstract interface for LLM providers.
 * Implementations handle the specifics of communicating with a particular
 * LLM service (e.g., Anthropic, OpenAI).
 */
export interface LLMProvider {
  /**
   * Send messages to the LLM and get a response.
   *
   * @param messages - Conversation history
   * @param tools - Available tools the LLM can call
   * @param systemPrompt - System prompt to guide LLM behavior
   * @returns Response from the LLM
   * @throws LLMError on failure
   */
  sendMessage(
    messages: LLMMessage[],
    tools?: Tool[],
    systemPrompt?: string,
    signal?: AbortSignal
  ): Promise<LLMResponse>;
}

// =============================================================================
// Error Types
// =============================================================================

/**
 * Error codes for LLM-related failures.
 * - auth_error: Invalid or missing API key
 * - rate_limit: Rate limited by the provider
 * - network: Network connectivity issue
 * - invalid_request: Malformed request (e.g., too many tokens)
 * - unknown: Unclassified error
 */
export type LLMErrorCode =
  | "auth_error"
  | "rate_limit"
  | "network"
  | "invalid_request"
  | "aborted"
  | "unknown";

/**
 * Error class for LLM-related failures.
 * Distinguishes between retryable and non-retryable errors to guide retry logic.
 */
export class LLMError extends Error {
  /**
   * Create a new LLMError.
   *
   * @param message - Human-readable error description
   * @param code - Error classification code
   * @param retryable - Whether this error might succeed on retry
   */
  constructor(
    message: string,
    public readonly code: LLMErrorCode,
    public readonly retryable: boolean
  ) {
    super(message);
    this.name = "LLMError";
    // Ensure proper prototype chain for instanceof checks
    Object.setPrototypeOf(this, LLMError.prototype);
  }

  /**
   * Create an authentication error (non-retryable).
   */
  static authError(message: string): LLMError {
    return new LLMError(message, "auth_error", false);
  }

  /**
   * Create a rate limit error (retryable with backoff).
   */
  static rateLimit(message: string): LLMError {
    return new LLMError(message, "rate_limit", true);
  }

  /**
   * Create a network error (retryable).
   */
  static network(message: string): LLMError {
    return new LLMError(message, "network", true);
  }

  /**
   * Create an invalid request error (non-retryable).
   */
  static invalidRequest(message: string): LLMError {
    return new LLMError(message, "invalid_request", false);
  }

  /**
   * Create an unknown error (non-retryable by default).
   */
  static unknown(message: string): LLMError {
    return new LLMError(message, "unknown", false);
  }

  /**
   * Create an aborted error (non-retryable).
   * Used when a request is cancelled by the user.
   */
  static aborted(message: string): LLMError {
    return new LLMError(message, "aborted", false);
  }
}

// =============================================================================
// Type Guards
// =============================================================================

/**
 * Type guard to check if a content block is a text block.
 */
export function isTextContentBlock(block: ContentBlock): block is TextContentBlock {
  return block.type === "text";
}

/**
 * Type guard to check if a content block is a tool use block.
 */
export function isToolUseContentBlock(block: ContentBlock): block is ToolUseContentBlock {
  return block.type === "tool_use";
}

/**
 * Type guard to check if a content block is a tool result block.
 */
export function isToolResultContentBlock(block: ContentBlock): block is ToolResultContentBlock {
  return block.type === "tool_result";
}

/**
 * Extract all tool calls from an LLM response.
 */
export function extractToolCalls(response: LLMResponse): ToolCall[] {
  return response.content.filter(isToolUseContentBlock).map((block) => ({
    id: block.id,
    name: block.name,
    input: block.input,
  }));
}

/**
 * Extract all text content from an LLM response as a single string.
 */
export function extractTextContent(response: LLMResponse): string {
  return response.content
    .filter(isTextContentBlock)
    .map((block) => block.text)
    .join("\n");
}
